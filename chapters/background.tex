\chapter{Background}
\label{chap:background}

Before presenting my work, I review in this chapter some of the foundations upon
which it rests.
First, I broadly discuss tactic languages in
Sec.~\ref{sec:tactics-tacticals} before moving on to languages used for
defining tactics in Sec.~\ref{sec:defining-tactics}.
Next, since Harpoon can be viewed also as a form of structured editor for
proofs, I survey the literature on structured editing in
Sec.~\ref{structured-editing}.
Finally, I discuss the Beluga project at large since Harpoon builds directly on
Beluga.

\section{Tactics and tacticals}
\label{sec:tactics-tacticals}

\subsection{Origins: the LCF system}

Now a common word in today's proof assistant jargon, the notion of \emph{tactic}
as we know it was introduced by Robin Milner nearly 40 years ago
as part of the Logic for Computable Functions (LCF) system
\cite{milner-tactics, lcf}.
This system is specialized for reasoning in a logic called \textsc{pplambda}, a
polymorphic predicate lambda-calculus, based on Dana Scott's Logic \emph{of}
Computable Functions \cite{scott-lcf}.
Although the way we use tactics has evolved somewhat since then, the core idea
remains remarkably the same: a tactic is a function applied to a goal to
eliminate it, producing zero or more new goals.
This view of tactics is natural if one understands an inference rule as a
function from theorems to theorems: a primitive tactic is merely the inverse of
an inference rule, mapping the conclusion of that rule to its necessary
premises.
As not every tactic is applicable to every goal, Milner defines a tactic
specifically as a partial function.
In addition, a tactic produces as output a function that Milner calls a
\emph{validation}.
This function accepts a list of theorems -- the eventual solutions to the new
goals generated by the tactic -- and produces a new theorem.
This is due to the crucially backwards orientation of the tactics-based
approach: tactics act on a \emph{goal} (a desired end state), so validations
are composed together once the proof is complete to form a forwards proof.

\newcommand{\theorem}{\mathtt{theorem}}
For example, a tactic representing the rule for $\land$-introduction
applied to a goal of the form $A \land B$ generates two subgoals, $A$ and
$B$.
The validation generated by the tactic expects a list of two theorems, one for
$A$ and one for $B$, and constructs the theorem $A \land B$ using the function
representing the $\land$-introduction rule,
of type $\theorem \to \theorem \to \theorem$.

The core tactics in LCF are simply the inference rules of \textsc{pplambda}, but
having only these would be quite limiting.
Constructing even small proofs would be a hugely laborious task in that case.
To make the tactic-based approach to proving more practical, Milner
introduces tactic combinators called \emph{tacticals}.
These can take advantage of tactics' failure for backtracking and
repetition.
As an example of the former, the tactic $t_1 \;\mathsf{Orelse}\; t_2$,
constructed using the $\mathsf{Orelse}$ tactical, executes $t_1$ and if it
fails, then $t_2$.
As an example of the latter, the tactic $\mathsf{Repeat}\; t$ executes the tactic
$t$.
On failure, nothing happens.
On success, $\mathsf{Repeat}\; t$ is executed on every subgoal generated by the
successful application of $t$.

A strength of the LCF system is that users may define their own tactics, but
this comes also at a cost.
Users must also therefore define the validation associated to their tactic.
Granting totally unrestricted power to the user in constructing theorems would
be a huge blow to any potential soundness guarantee for the system.
Instead, only limited power is given to the user, as Milner says that
``the only operations for generating [theorems] are the basic
inference rules ... and the rules derived from them.''
But this isn't quite enough either.
Nothing (statically) prevents a user from defining what Milner calls an
``invalid'' tactic.
%
He writes in \cite{lcf},
\begin{quote}
  Validity is clearly a necessary condition for a tactic to be useful; indeed we
  may deny that invalid tactics are tactics at all.  But it is hard to see how
  to design a programming language so that all definable objects of type
  \texttt{tactic} are valid, or how to gain this effect by a type discipline.
  At most we can adopt a style which encourages the programming of valid
  tactics; this can be done with tacticals.
\end{quote}
%
Milner shows on paper that given valid tactics, his predefined tacticals
generate valid tactics. But to reiterate, the overall soundness of the system is
undermined by the user's ability to define and use invalid tactics and
tacticals.

\inlinetodo{%
  Is there more to say? Some kind of conclusion for this section?%
}

\subsection{Tactics in Isabelle}

\newcommand{\isb}{Isabelle-88}

In the early 80's, Lawrence Paulson assists Milner in the development of the
LCF system in Edinburgh. Once Paulson's time in Scotland ends, he travels
south to England, taking with him the insights from the LCF system to create
Cambridge LCF, which goes on to become the Isabelle-86 and later the \isb{}
system \cite{isabelle-origin}.

Both \isb{} and Edinburgh LCF use ML as a form of interactivity with the
user.
That is, theorems can be proven interactively purely via the ML
read-eval-print loop (REPL).
The main downside to this approach is that anything is possible:
this REPL allows general-purpose programming in ML, of which one potential
application is the development of proofs.
In other words, \isb{} is an ML library rather than an application per se.
The functions representing tactics must be applied manually to
values representing goals to build new goal values.
These entities must all be managed directly by the user.

To alleviate some of this manual labour, a helper library called the
\emph{goal stack package} is bundled together with \isb{} that automates much of
the goal management.
It provides a notion of a current goal state, an undo mechanism, and a helper
for invoking a tactic on the current state.
This helper adds any new goals generated by the tactic to the goal stack.

\isb{} builds substantially on Edinburgh LCF.
First, whereas LCF is specialized for reasoning about a particular logic called
\textsc{pplambda}, \isb{} aims to be a \emph{generic} theorem prover, capable of
reasoning about several logics that one would encode in it\footnotemark.
Second, as for the tactic languages, one key development in \isb{} is the
removal of validations, those functions used to construct a forwards proof
once the tactic-based backwards proof is complete.
%
\footnotetext{%
  A similar drive to represent represent and reason about a broader class of
  logics also takes place in Edinburgh, culminating with the development of the
  Edinburgh logical framework LF.%
}
%
Recall that these validations are a source of concern in Edinburgh LCF as users
can define invalid tactics, whose validations construct a proof for the the
wrong theorem.
A shift in the representation technique for inference rules is what enables
explicit, manual validations to be eliminated.
Rather than represent inference rules as opaque functions, they are represented
directly as data in the system.
From this representation, both forwards reasoning functions as in LCF and
backwards reasoning tactics can be derived.
Moreover, the application of a tactic automatically derived from an inference
rule performs the work of LCF's validations behind the scenes as what Paulson
calls a ``meta-inference''.
Since a new object-logic is encoded by extending \isb's meta-logic
\newcommand{\M}{$\mathcal{M}$}%
\M{} with additional axioms representing as
(meta-)implications the inference rules of the object-logic,
reasoning in the metalanguage \M{} amounts to reasoning in the object-logic.
In fact, an entire intermediate \emph{proof state} for a theorem in an object
language is represented as a \emph{theorem} in \M.
Hence, it is by checking inferences \emph{in \M{}} that the system ensures that
every proof state is arrived at correctly.
Users may define new tactics, but these, by virtue of ultimately using the
primitive tactics representing inference rules, effectively represent
\emph{derived} inference rules in the object language.

Although the underlying details of tactics are quite different, the tactics and
tacticals themselves are mostly the same, and remain quite powerful.
Tacticals for repetition, backtracking, and so on are all present in
\isb.
Paulson demonstrates the power of tacticals by using them to build a proof
search strategy for classical first-order logic.
Paulson's strategy is to divide inference rules into two categories
that he calls ``safe'' and ``unsafe''.
A rule that can be applied eagerly, without affecting the provability of the
overall statement, is safe; else it is unsafe.
Then, one applies as many safe rules as possible until applying one unsafe rule,
and repeating.
This technique would later be further developed by Andreoli, in the setting of
linear logic, where it is called ``focusing'' and where the categories of rules
are called ``asynchronous'' and ``synchronous'' \cite{andreoli-logic-1992}.
That Paulson be able to implement such a powerful proof search technique using
only a handful of combinators speaks to the strength of tacticals.

Now it seems that there are two different views of tactics.
Some tactics represent (derived) inference rules in the object-logic, whereas
others represent proof search and automation techniques.
It is not immediately clear that these two views can be reconciled.
Inference rules have clear conditions under which they may apply, and their
outcomes are predictable, whereas proof search techniques have less clear
applicability conditions and their outcomes can be uncertain.
These two different views were already present in LCF, and we will see that
these two views appear in later systems, too.

\inlinetodo{%
  Discuss that the tactic language is low-level. Subgoals are referred to by
  number.
}

\inlinetodo{%
  Discuss that tactics in isabelle emit not only a \emph{list} of subgoals,
  but a potentially infinite stream of subgoals.
  Also, it's super unclear how that works at all: does one need only solve
  \emph{one} of these subgoals? Or all of them?  Or is the stream purely for
  nondeterminism in the proof search?
}

\section{Languages for defining tactics}
\label{sec:defining-tactics}

\isb{} and LCF do not have proper languages for defining tactics.
Instead, one defines tactics essentially by extending the proof assistant
itself.
This works reasonably well in those settings because the assistant is
interpreted, and we can think of the assistant not as an application itself, but
rather as a library implemented in ML.
The upshot is that one can use the ML REPL to develop proofs interactively.

When a proof assistant is its own, separate application, then the development of
new tactics cannot reasonably proceed by extending the assistant itself.
Therefore, we see a drive to design languages for defining the new tactics.

\subsection{Ltac in Coq}

In the mid-80s, as LCF and Isabelle are developed in Scotland and England,
Thierry Coquand and Gérard Huet develop the Calculus of Constructions in France
\cite{coc}.
This is a higher-order constructive logic possessing all forms of quantification
(in the sense of Barendregt's lambda cube \cite{barendregt-lambda}).
Furthermore, they developed an implementation of this logic, called
Coq\footnotemark.
What distinguishes Coq from LCF and Isabelle is that it is designed with
\emph{program extraction} in mind.
This technique recovers from a proof object its computational component as an
executable program in a conventional programming language, e.g. ML.
%
\inlinetodo{%
  Discuss the addition of inductive families in the Calculus of Inductive
  Constructions?
}

\footnotetext{%
  The system was itself called \textsc{CoC} and then \textsc{constr} for several
  years before being renamed, but I will refer to it only as Coq for simplicity.
}

As a highly simplified but concrete example, consider a $\Pi_1$ formula, of the
form $\forall x{:}\tau_1.\exists y{:}\tau_2.P$.
In constructive logic, existential claims actually contain an object that
witnesses the claimed existence.
Then it seems possible to transform this logical statement into a program that,
given an $x{:}\tau_1$, computes the $y{:}\tau_2$ that witnesses the claim.
That is, one can recover an ordinary function of type $\tau_1 \to \tau_2$.
\inlinetodo{Say something about Curry-Howard and erasure of computationally
  irrelevant parts of the proof term?}

This extraction process enables the development of \emph{verified algorithms}.
First, one uses the system to prove a theorem, e.g. ``for any list $l$, there
exists a list $l^\prime$ such that $l^\prime$ contains exactly the elements of
$l$ and such that $l^\prime$'s elements are in ascending order''.
Then, one uses program extraction to obtain from this theorem a proper sorting
algorithm together with a guarantee of this algorithm's correctness.

Similar to LCF and Isabelle, Coq features a number of tactics and tacticals to
aid in proof development.
The original tactics are much like LCF's and Isabelle's, so I will not describe
them.
An important difference, however, between Coq and the other systems I have
described, is that Coq is a proper application written in ML\footnotemark,
with its own syntax, parser, etc.
%
\footnotetext{%
  Coq has been implemented in many different ML languages over the years, first
  in CAML, then in Caml-light, and finally (and still to this day) in OCaml.
  To simplify, I will say that Coq is merely implemented in ML.
}
%
In other words, users do not interact with Coq via the ML REPL.
This has two unfortunate downsides for the extensibility of Coq's tactic
language.
First, it is challenging to add new tactics to Coq, as one must obtain the
source code for Coq, implement the new tactics in ML, and recompile the system
as a whole.
Second, the distribution of custom tactics is difficult: should one ask that
one's domain-specific tactic be included in the core system, or should one
circulate a patch file to be applied to the system's source tree?
Neither of these distribution strategies is particularly satisfying.
Domain specific tactics should probably not be included in the core distribution
of the system, else users eventually find themselves drowning in an abundance of
highly specialized tactics unnecessary to their specific problem domain.
The circulation of patches, on the other hand, requires that one keep their
patches constantly up to date with the current distributed version of the
system.

At last, a solution appears at the turn of the millenium as David Delahaye
announces the Ltac language \cite{Delahaye:LPAR00}.
Ltac is a domain-specific language embedded within Coq for defining new
tactics.
Essentially, scripts in Ltac are interpreted to execute commands in Coq's
kernel.
The benefits of Ltac are that it is high-level compared to implementing tactics
directly in ML and that new tactics can be defined alongside proofs.
In fact, Delahaye reports significant code size reductions (and even sometimes
speedups!) in tactics ported from ML to Ltac.
Since custom tactics can be defined together with one's proofs, tactic and even
proof distribution are vastly simplified.
Indeed, Pierre Pédrot later writes in \cite{ltac2}, ``the Ltac tactic language
is probably one of the major ingredients of the success of [Coq], as it allows
to write proofs in an incremental, more efficient and more robust way that the
state of the art at that time.''

To illustrate one particularly high-level construct from Ltac called
\texttt{match goal}, let us consider an example from Delahaye's paper, in which
he proves that the natural numbers have more than two elements\footnotemark.
This statement can be expressed formally as
\newcommand{\N}{\mathbb{N}}%
\[
  \neg (\exists x{:}\N.\exists y{:}\N.\forall z{:}\N.\, x = z \lor y = z)
\]
%
\footnotetext{%
  In the version of Ltac presented by Delahaye, the construct is called
  \texttt{Match Context}, and overall his example no longer works with the
  latest version of Coq.
  I will discuss an updated version of the example that I have produced.
  See Appendix~\ref{app:delahaye-example-contrast} for the original and updated
  versions of the example.
}
%
The proof stategy is to assume the negated formula and arrive at a
contradiction.
The assumed existential claim can be decomposed, assuming the existence of such
an $x$ and such a $y$.
Then, it suffices to instantiate the assumption $\forall z{:}\N. x = z \lor y =
z$ with three distinct numbers, say $1$, $2$, and $3$.
At this point, we have three relevant assumptions:
\renewcommand{\H}{\mathcal{H}}
$\H_1 :: x = 1 \lor y = 1$,
$\H_2 :: x = 2 \lor y = 2$, and
$\H_3 :: x = 3 \lor y = 3$.
Successively eliminating these assumptions can be accomplished quite easily with
the \texttt{;} (semicolon) tactical, which applies the tactic on the right to
each subgoal generated by the tactic on the left:
\newcommand{\elim}{\mathtt{elim}\;}
$\elim \H_1;\; \elim \H_2;\; \elim \H_3$.
In detail, the first elimination generates two subgoals, and \emph{for each} of
these, the second elimination generates two subgoals,
\emph{and for each of those}, the third elimination generates two subgoals.
This exponential blowup results in eight subgoals, each of which contains a
pair of assumptions of the form $x = a$ and $x = b$, or $y = a$ and $y = b$ for
distinct constants $a$, $b$.

Ltac's \texttt{match goal} construct is a form of pattern matching for
extracting parts of the active subgoal.
One can match on available assumptions as well as on the current goal.
The pattern Delahaye uses is \texttt{[_ : ?x = ?a, H : ?x = ?b |- _]}.
The turnstile separates the hypothesis pattern from the goal pattern.
The question mark syntax expresses metavariables, and the nonlinear appearance
of \texttt{?x} will involve unification during matching.
This pattern can match all eight of the subgoals that we have generated, as
each of them contain (at least) the assumptions of the written forms.
Then, in the body of this branch, we eliminate the first equality using
\texttt{subst x}, refining the type of \texttt{H} to evidently be uninhabited,
as it states an equality between two syntactically unequal, closed terms.
Then we eliminate the absurd assumption \texttt{H} using the
\texttt{discriminate} tactic to produce the required contradiction.
Since we want to perform this general analysis on all eight subgoals, we
sequentially compose the \texttt{match goal} construct with the eliminations
using \texttt{;}.
See Fig.~\ref{fig:ltac-example} for the full example.

\begin{figure}[t]
  \lstinputlisting[language=Coq]{snippets/nat-card.v}%
  \caption{%
    A simple theorem on the cardinality of natural numbers, to illustrate the
    \texttt{match goal} construct in Ltac.%
  }
  \label{fig:ltac-example}
\end{figure}

The beauty of Ltac is that this basic recipe could be further generalized to
prove that the natural numbers have more than $n$ elements for a closed $n$.
First eliminate the $n$ nested existentials. Second, eliminate the universal
$n + 1$ times, with distinct naturals, e.g. $0$ through $n$.
Finally, end the proof in the same way, using \texttt{match goal} to find a pair
of equalities that can be used to produce a contradiction.

The main downside to Ltac is that it is ill-specified.
Indeed, Delahaye does not give any semantics for tactics defined in
Ltac, and upon Ltac's release, it was not typed.
The lack of a typing discipline would not, however, be a dealbreaker, given that
Ltac runs during typechecking anyway:
any runtime error during execution of an Ltac script becomes a
type error at its invocation site.
Overall, the lack of specification is acceptable at the time of Ltac's release
as the language eliminates a growing problem with custom tactics by allowing
users to define simple custom tactics while encouraging them to implement
larger, more sophisticated ones in ML.

Alas, it is all too common for domain-specific languages to outgrow their
original scope, and Ltac is no exception.
Over the following two decades, several other approaches to programming
tactics in Coq are proposed: Mtac~\cite{mtac-journal}, Rtac~\cite{rtac},
Mtac2~\cite{mtac2}, and Template-Coq~\cite{template-coq} to name a few.
% What these proposed tactic languages have in common is that they are quite
% unlike Ltac:


\section{Structured editing}
\label{sec:structured-editing}

\section{Beluga}
\label{sec:beluga-intro}

%
%%% Local Variables:
%%% TeX-master: "../main.tex"
%%% End:
